{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiNUDdaYlzEs"
   },
   "source": [
    "# **Roll No : 19BCE102**\n",
    "# **Data Mining - 2CSDE71**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4I34f-NUp8f"
   },
   "source": [
    "# Practical-9 Clustering using TF-IDF & Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzF6lACjNXB6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QLlGdGUb2G3"
   },
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cY8VFh2oNecN"
   },
   "outputs": [],
   "source": [
    "corpus = \"\"\"\n",
    "Simple example with Cats and Mouse\n",
    "Another simple example with dogs and cats\n",
    "Another simple example with horse and chana\n",
    "\"\"\".split(\"\\n\")[1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjmxhuMxb6PR"
   },
   "source": [
    "### Tf-Idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RC17gqA9QPis",
    "outputId": "bb22e5b8-a2a6-44e1-c69c-97ba95a58578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary generated - :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['mouse',\n",
       " 'and',\n",
       " 'simple',\n",
       " 'cats',\n",
       " 'chana',\n",
       " 'another',\n",
       " 'with',\n",
       " 'example',\n",
       " 'horse',\n",
       " 'dogs']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenization and vocabulary generation\n",
    "vocab = []\n",
    "\n",
    "for idx,cur_sent in enumerate(corpus):\n",
    "  corpus[idx] = cur_sent.lower().split(\" \")\n",
    "  vocab += corpus[idx]\n",
    "\n",
    "vocab = list(set(vocab))\n",
    "print(\"Vocabulary generated - :\")\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMYzgsl-U_u_"
   },
   "outputs": [],
   "source": [
    "# Generating dictionary of word for each sentence\n",
    "word_dict = []\n",
    "for idx,cur_sent in enumerate(corpus):\n",
    "  cur_word_dict = dict()\n",
    "  for cur_word in vocab:\n",
    "    cur_word_dict[cur_word] = cur_sent.count(cur_word)\n",
    "  word_dict.append(cur_word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RduEBEcpBwan",
    "outputId": "19e13075-4e41-4176-d190-6f77dcbd9798"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'mouse': 1, 'and': 1, 'simple': 1, 'cats': 1, 'chana': 0, 'another': 0, 'with': 1, 'example': 1, 'horse': 0, 'dogs': 0}, {'mouse': 0, 'and': 1, 'simple': 1, 'cats': 1, 'chana': 0, 'another': 1, 'with': 1, 'example': 1, 'horse': 0, 'dogs': 1}, {'mouse': 0, 'and': 1, 'simple': 1, 'cats': 0, 'chana': 1, 'another': 1, 'with': 1, 'example': 1, 'horse': 1, 'dogs': 0}]\n"
     ]
    }
   ],
   "source": [
    "print(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65EsZb1LYjV2"
   },
   "outputs": [],
   "source": [
    "def compute_tf(word_dict, l):\n",
    "    tf = {}\n",
    "    sum_nk = len(l)\n",
    "    for word, count in word_dict.items():\n",
    "        tf[word] = count/sum_nk\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEGjCM7vZpt1"
   },
   "outputs": [],
   "source": [
    "def compute_idf(strings_list):\n",
    "    n = len(strings_list)\n",
    "    idf = dict.fromkeys(strings_list[0].keys(), 0)\n",
    "    for l in strings_list:\n",
    "        for word, count in l.items():\n",
    "            if count > 0:\n",
    "                idf[word] += 1\n",
    "    \n",
    "    for word, v in idf.items():\n",
    "        idf[word] = math.log(n / float(v))\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S96DvDjaZrDt"
   },
   "outputs": [],
   "source": [
    "def compute_tf_idf(tf, idf):\n",
    "    tf_idf = dict.fromkeys(tf.keys(), 0)\n",
    "    for word, v in tf.items():\n",
    "        tf_idf[word] = v * idf[word]\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4wGehy1Zz4V"
   },
   "outputs": [],
   "source": [
    "tf_list = []\n",
    "for idx,cur_dict in enumerate(word_dict):\n",
    "  tf_list.append(compute_tf(cur_dict, corpus[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrhssWoeakYd"
   },
   "outputs": [],
   "source": [
    "idf = compute_idf(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yk4gQRGXBwas",
    "outputId": "b3f87873-f727-4b71-bf9f-4d27af284114"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mouse': 1.0986122886681098,\n",
       " 'and': 0.0,\n",
       " 'simple': 0.0,\n",
       " 'cats': 0.4054651081081644,\n",
       " 'chana': 1.0986122886681098,\n",
       " 'another': 0.4054651081081644,\n",
       " 'with': 0.0,\n",
       " 'example': 0.0,\n",
       " 'horse': 1.0986122886681098,\n",
       " 'dogs': 1.0986122886681098}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41O9gKn2asFV"
   },
   "outputs": [],
   "source": [
    "tf_idf_list = []\n",
    "for idx,cur_tf in enumerate(tf_list):\n",
    "  tf_idf_list.append(compute_tf_idf(cur_tf, idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6AufANSSdQ2e",
    "outputId": "665b3acb-2caa-43cc-f694-d85aa71fa41e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.1831020481113516, 0.0, 0.0, 0.06757751801802739, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_list[0].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMHoqjNBelUX"
   },
   "outputs": [],
   "source": [
    "tf_idf_vectors = []\n",
    "\n",
    "for idx,cur_tf in enumerate(tf_list):\n",
    "  tf_idf_vectors.append(list(tf_idf_list[idx].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_L6XBgvSg7xJ"
   },
   "outputs": [],
   "source": [
    "tf_idf_vectors = np.array(tf_idf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYEOMaB3Bwav",
    "outputId": "b55f4c82-ab17-40d1-b8a3-0cea7e41996a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18310205, 0.        , 0.        , 0.06757752, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.05792359, 0.        ,\n",
       "        0.05792359, 0.        , 0.        , 0.        , 0.15694461],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.15694461,\n",
       "        0.05792359, 0.        , 0.        , 0.15694461, 0.        ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zbuQvA4cA02"
   },
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcn2tyLFdFrl"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist \n",
    "def kmeans(x,k, no_of_iterations):\n",
    "    idx = np.random.choice(len(x), k, replace=False)\n",
    "    #Randomly choosing Centroids \n",
    "    centroids = x[idx, :] #Step 1\n",
    "     \n",
    "    #finding the distance between centroids and all the data points\n",
    "    distances = cdist(x, centroids) #Step 2\n",
    "     \n",
    "    #Centroid with the minimum Distance\n",
    "    points = np.array([np.argmin(i) for i in distances]) #Step 3\n",
    "     \n",
    "    #Repeating the above steps for a defined number of iterations\n",
    "    #Step 4\n",
    "    for _ in range(no_of_iterations): \n",
    "        centroids = []\n",
    "        for idx in range(k):\n",
    "            #Updating Centroids by taking mean of Cluster it belongs to\n",
    "            temp_cent = x[points==idx].mean(axis=0) \n",
    "            centroids.append(temp_cent)\n",
    " \n",
    "        centroids = np.vstack(centroids) #Updated Centroids \n",
    "         \n",
    "        distances = cdist(x, centroids ,'euclidean')\n",
    "        points = np.array([np.argmin(i) for i in distances])\n",
    "         \n",
    "    return points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fcaoW-YifP9d"
   },
   "outputs": [],
   "source": [
    "kmeans_labels = kmeans(tf_idf_vectors,2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ohMA1LXfDiT",
    "outputId": "d2a45079-6cbe-41a5-9bf9-54a4bc2a901b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The labels generated by kmeans algorithm :\n",
      "[0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(f\"The labels generated by kmeans algorithm :\")\n",
    "print(kmeans_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cE-Px2pckJJV",
    "outputId": "6497e447-a5d6-4441-d0b1-f612ec4f32a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence belong to class 0 are : \n",
      "simple example with cats and mouse\n",
      "another simple example with dogs and cats\n",
      "\n",
      "\n",
      "sentence belong to class 1 are : \n",
      "another simple example with horse and chana\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "  print(f\"sentence belong to class {i} are : \")\n",
    "  for idx,cur_sent in enumerate(corpus):\n",
    "    if kmeans_labels[idx] == i:\n",
    "      print(\" \".join(cur_sent))\n",
    "  print()\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zz5CR0skU8vo"
   },
   "source": [
    "## **CONCLUSION**\n",
    "Term Frequency-Inverse Document Frequency is a numerical statistic that demonstrates how important a word is to a corpus. We can compute TF_IDF vectors and use Kmeans clustering to cluster cluster documents and articles."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "19bce257_practical-9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
